\section{Implementación}

Respecto a la implementación, el software para balancear las clases está implementado en Python mientras que las ejecuciones experimentales se han realizado en R utilizando la biblioteca RKEEL.

% TODO: Explicar el algoritmo de Bojarczuk_GP

\subsubsection{Otras funciones auxiliares}

También disponemos de algunas funciones auxiliares, que nos servirán a lo largo de todo el código, como la comprobación de si dos valores reales son iguales teniendo en cuenta los problemas de representación de números en coma flotante de C++, o distintas posibles funciones de evaluación que hemos predefinido.

\newpage

\subsection{Validación de las pruebas}


Uno de los problemas que nos encontramos en los algoritmos de aprendizaje automático es la validación del algoritmo, es decir, como podemos asegurar que nuestro algoritmo realmente funciona, y no se trata de que ha hecho un sobreajuste con los datos con los que ha entrenado y fuera de dichos datos no es capaz de realizar buenas predicciones.

Para comprobar y asegurarnos que los algoritmos son capaces de realizar buenas predicciones fuera de los datos con los que ha entrenado utilizaremos validación cruzada con $k$ iteraciones.

Este método consiste en dividir el conjunto de entrenamiento en $k$ partes de mismo tamaño, de forma que realizaremos $k$ iteraciones para entrenar nuestro modelo. En cada iteración utilizaremos una de estas partes como conjunto de validación y las otras $k - 1$ partes como conjunto de entrenamiento, de forma que el algoritmo no haya entrenado con esa parte de validación. Finalmente para obtener el error del algoritmo para dicho conjunto de datos utilizaremos la media del error en los distintos conjuntos de validación de cada iteración.

Es importante destacar que antes de hacer las separaciones en $k$ subconjuntos de datos el conjunto de entrenamiento se reordenará de forma aleatoria para evitar que un conjunto en el que los datos estén ordenados por clase se excluya del entrenamiento cierta clase, porque todas sus muestras están en el conjunto de validación.

De esta forma podremos obtener un valor del error del algoritmo para unos datos con los que no ha entrenado, a la vez que comprobamos que nuestro algoritmo es capaz de ajustarse bien o no a un conjunto de datos.


\begin{figure}[H]
    \centering

	 \begin{subfigure}[b]{\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{cross_validation/conjunto_datos.png}
		\caption{Ejemplo de un conjunto de 16 datos.}
	  \label{fig:ej_16_datos}
   \end{subfigure}
	\vspace{1cm}

	 \begin{subfigure}[b]{\textwidth}
		 \centering
		 \includegraphics[width=0.8\textwidth]{cross_validation/iteracion1.png}
 		 \caption{Separación entre validación y entrenamiento en la primera iteración.}
 	    \label{fig:cv_iteracion1}
	 \end{subfigure}
	 \vspace{1cm}

	\begin{subfigure}[b]{\textwidth}
		 \centering
		 \includegraphics[width=0.8\textwidth]{cross_validation/iteracion2.png}
 		 \caption{Separación entre validación y entrenamiento en la segunda iteración.}
 	    \label{fig:cv_iteracion2}
   \end{subfigure}
	\vspace{1cm}

	\begin{subfigure}[b]{\textwidth}
		 \centering
		 \includegraphics[width=0.8\textwidth]{cross_validation/iteracion3.png}
 		 \caption{Separación entre validación y entrenamiento en la tercera iteración.}
 	    \label{fig:cv_iteracion3}
	\end{subfigure}
	\vspace{1cm}

	\begin{subfigure}[b]{\textwidth}
		 \centering
		 \includegraphics[width=0.8\textwidth]{cross_validation/iteracion4.png}
 		 \caption{Separación entre validación y entrenamiento en la cuarta iteración.}
 	    \label{fig:cv_iteracion4}
	\end{subfigure}

	\caption{Ejemplo de validación cruzada de 4 iteraciones.}
	\label{fig:4-cv-ejemplo}
\end{figure}


\subsubsection{Validación cruzada utilizando 5x2cv}

En nuestro caso utilizaremos 5x2cv para validación. Este método fue propuesto por Thomas G. Dietterich en el año 1998 \cite{propuesta5x2cv}.

Este método se basa en realizar cinco repeticiones de una validación cruzada con dos iteraciones, cada una de estas repeticiones utilizando particiones distintas de los datos al 50\%. De esta forma, con cada una de las cinco particiones que realicemos a los datos aplicaremos una validación cruzada con dos folds, entrenando con la primera mitad de los datos y validando con la segunda mitad en el primer fold, y al contrario en el segundo fold.

Como podemos ver, con cinco repeticiones y siendo cada una de estas un 2-cv (de ahí el nombre 5x2cv) obtendremos diez valores de error, siendo el error final obtenido en validación la media de estos diez valores.



\newpage

\subsection{Funciones de evaluación}

De cara a evaluar los algoritmos necesitaremos una función de evaluación que nos permita saber el error que estamos cometiendo en las estimaciones.

En nuestro caso, al estar ante un problema de clasificación utilizaremos las siguientes:

\begin{itemize}
	\item Precisión.
	\item Área bajo la curva ROC.
	\item Error absoluto medio ordinal.
\end{itemize}

\subsubsection{Precisión}


\subsubsection{Área bajo la curva ROC}


\subsubsection{Error absoluto medio ordinal}



\newpage

\subsection{Documentación del proyecto}

Todo el proyecto se ha documentado con Doxygen, y se puede acceder a la documentación a través del fichero \texttt{index.html} de la carpeta \texttt{docs/} en el código o bien en la web \url{https://advy99.github.io/algoritmos_poblacion_expresiones/}, ya que se ha utilizado la herramienta GitHub Pages \cite{GHPages} para alojar la documentación del proyecto.

En este documento se puede encontrar la estructura del proyecto, así como las distintas opciones de compilación.

\begin{figure}[H]
	 \centering
	 \includegraphics[width=\textwidth]{documentacion.png}
	 \caption{Página principal de la documentación del proyecto.}
	\label{fig:documentación}
\end{figure}

\newpage

\subsection{Verificación del software desarrollado}

De cara a verificar que las distintas clases y métodos desarrollados funcionan utilizaremos GoogleTest \cite{gtest}.

GoogleTest es un entorno de trabajo para verificar software a través de test unitarios.

Los test unitarios son test que evalúan una sección concreta de un software, con el objetivo de comprobar que dicha sección de código funciona de forma correcta, y de esta forma aislar las secciones de código que no se comportan como esperábamos, facilitando la tarea de prueba y corrección de errores en un código.

Para que estos tests nos sean útiles tienen que cumplir las siguientes condiciones:

\begin{enumerate}
	\item Tienen que ser independientes y repetibles. El resultado de un test no debe depender de otros test, de forma que si un test falla este falle por la sección de código que estamos probando, no porque existe un test que no se comporta como creíamos.
	\item Tienen que estar estructurados, y reflejar como funciona todo el conjunto del software. De esta forma es sencillo separar como funcionan las distintas partes del software, permitiendo centrarnos en comprobar que cada parte funciona de forma correcta.
	\item Deben de ser portables y reutilizables. Los tests deben finalizar correctamente independientemente del sistema y compilador utilizado.
	\item Cuando un test falla debe proporcionar tanta información como le sea posible.
	\item Los test han de ser rápidos. El objetivo es comprobar que el software funciona como queremos de una forma rápida, sin perder más tiempo en ejecutar los tests que en compilar y ejecutar el sistema.
\end{enumerate}

Siguiendo estas premisas, se han diseñado tests para cada una de las clases y el conjunto del sistema software. Dichos tests están disponibles en la carpeta \texttt{test} del software, y además el fichero para generar el software ejecutará dichos tests, de forma que si estos tests no son completados correctamente detendrá la compilación.

\newpage

\subsection{Como utilizar y compilar el software}

Existen diversas formas de compilar, todas ellas utilizando el comando `make`.

\begin{itemize}
	\item Sin parámetros (\texttt{make}): Compilar con optimización y utilizando OpenMP para añadir paralelismo.
	\item Target \texttt{test} (\texttt{make test}): Compilar los test de GTest.
	\item Target \texttt{doc} (\texttt{make doc}): Compilar la documentación.
	\item Target \texttt{clean} (\texttt{make clean}): Limpiar los objetos tras una compilación.
	\item Variable \texttt{OPTIMIZACION} (\texttt{make OPTIMIZACION=3}): Por defecto 3, asignar un nivel de compilación. Los posibles valores son 0, 1, 2, 3 y g.
	\item Variable \texttt{OPENMP} (\texttt{make OPENMP=1}): Por defecto 1. Activar o desactivar la compilación con OpenMP, 0 desactivado, 1 activado.
	\item Variable \texttt{DEBUG} (\texttt{make DEBUG=0}): Por defecto 0. Activar o desactivar la compilación con símbolos de depuración.
	\item Variable \texttt{GPROF} (\texttt{make GPROF=0}): Por defecto 0. Activar o desactivar la compilación con GPROF de cara a hacer profiling al software.

\end{itemize}

Siempre que se compile se recompilará la documentación, de cara a posibles cambios.

Todas las variables se pueden combinar, por ejemplo, es posible cambiar el nivel de optimización y a la vez desactivar OpenMP.

Los binarios se generarán en la carpeta \texttt{bin} y cada uno de ellos tiene instrucciones de como utilizarlos.

\newpage

\subsection{Ejecución con un conjunto de datos de prueba}

De cara a probar que nuestra implementación funciona vamos a utilizar un conjunto de datos sencillo, con pocos datos y cuyo funcionamiento es muy simple, Iris \cite{irisDataset}. Este dataset contiene tres características de tres variantes de la flor Iris: Setosa, Versicolour y Virginica. Existen 50 muestras de cada clase, siendo 150 muestras en total.

% TODO: lanzar iris como conjunto de prueba


\newpage

\subsection{Uso de paralelismo}

Una de las tareas más costosas que nos podemos encontrar en ambos algoritmos es evaluar la población cuando esta tiene un gran número de individuos y el número de datos es muy elevado.

Para reducir el tiempo utilizado por estas tareas podemos aprovechar el paralelismo de los datos. Evaluar una expresión es totalmente independiente de evaluar otra, de forma que esta tarea se puede realizar en paralelo y así reducir el tiempo de ejecución.

También se ha probado el paralelizar la generación de la nueva población en cada generación del algoritmo, ya que en las operaciones de cruce y mutación son independientes entre sí al seleccionar a priori que elementos formarán parte del cruce y mutación, sin embargo esto no ha dado buenos resultados ya que el coste de crear y eliminar los hilos que se ejecutarán en paralelo, sumado a las esperas en secciones críticas esto hacía que el tiempo de ejecución fuera igual o superior a ejecutarlo en secuencial.

\newpage
